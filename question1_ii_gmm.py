# -*- coding: utf-8 -*-
"""Question1_ii_GMM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IEXTf4mQvudAkl_Md3bO3Qt4VTTe0Wi-
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.ticker as ticker
# import seaborn as sns
from numpy import linalg as LA

df = pd.read_csv("A2Q1.csv",header=None)
np_df=df.to_numpy()

#GAUSSIAN MIXTURE MODEL

num_clusters = 4
num_iterations = 5

# Initializing params for EM algorithm
def inititialize_params(num_clusters):
  mu_k = np.random.uniform(0., 3., num_clusters).reshape(1, -1)
  sigma_k_sq = np.random.uniform(0.05, 1, num_clusters).reshape(1, -1)

  pi_k = np.random.dirichlet(np.ones(num_clusters)*100, size=1)
  return mu_k, sigma_k_sq, pi_k


# Expectation step
def expectation(mu_k, sigma_k_sq, pi_k, X):
  sigma_k=np.sqrt(sigma_k_sq)
  numerator= np.reciprocal((np.sqrt(2 * np.pi)*sigma_k))* np.exp(-0.5*(((X-mu_k)/sigma_k))**2) * pi_k
  denominator=np.sum(numerator, axis=1).reshape(-1, 1)
  lambda_ik = numerator/denominator
  return lambda_ik, numerator


#function for updating pi_k used by maximization step
def update_pi_k(number_of_datapoints,lambda_ik):
  pi_k_updated = np.sum(lambda_ik, axis=0).reshape(1,-1)/ number_of_datapoints
  return pi_k_updated


#function for updating mu_k used by maximization step
def update_mu_k(lambda_ik, X):
  mu_k_updated= (np.dot(X.T,lambda_ik)/np.sum(lambda_ik, axis=0).reshape(1,-1)).reshape(1,-1)
  return mu_k_updated


#function for updating p_k used by maximization step
def update_sigma_k_sq(lambda_ik, X, mu_k):
  sigma_k_sq_updated= np.sum((((X-mu_k)**2)*lambda_ik),axis=0)/(np.sum(lambda_ik, axis=0).reshape(1,-1)).reshape(1,-1)
  return sigma_k_sq_updated


# Maximization step: Updating parameters using lambdas calculated in Expectation step
def maximization(lambda_ik, X):
  num_datapts = X.shape[0]
  pi_k = update_pi_k(num_datapts,lambda_ik)
  mu_k = update_mu_k(lambda_ik, X)
  sigma_k_sq = update_sigma_k_sq(lambda_ik, X, mu_k)
  return pi_k,mu_k,sigma_k_sq


def get_log_likelihood(lambda_ik,intermediate_term):
  log_intermediate_term = np.log(np.sum(intermediate_term, axis=1))
  lambda_k=np.sum(lambda_ik,axis=1)
  log_likelihood = np.sum(lambda_k * (log_intermediate_term - np.log(lambda_k)))
  return log_likelihood


def check_nans(mu_k, sigma_k_sq, pi_k):  

  if np.any(np.isnan(mu_k)):
    mu_k[np.where(np.isnan(mu_k))] = float(np.random.uniform(0.05, 1, 1))
  if np.any(mu_k < 1e-3):
    mu_k[np.where(mu_k < 1e-3)] = float(np.random.uniform(0.05, 1, 1))

  if np.any(np.isnan(sigma_k_sq)):
    sigma_k_sq[np.where(np.isnan(sigma_k_sq))] = float(np.random.uniform(0.05, 1, 1))
  if np.any(sigma_k_sq < 1e-3):
    sigma_k_sq[np.where(sigma_k_sq < 1e-3)] = float(np.random.uniform(0.05, 1, 1))

  if np.any(np.isnan(pi_k)):
    pi_k[np.where(np.isnan(pi_k))] = float(np.random.uniform(0.05, 0.5, 1))
  if np.any(pi_k < 1e-3):
    pi_k[np.where(pi_k < 1e-3)] = float(np.random.uniform(0.05, 0.5, 1))

  return mu_k, sigma_k_sq, pi_k

def EM_Algorithm(num_iterations,X,num_rand_initialisation=100):
  X = X.reshape(-1, 1)
  hundred_iter_loglikelihood = []
  hundred_iter_mu_k = []
  hundred_iter_sigma_k_sq = []

  for i in range(num_rand_initialisation):
    log_likelihood_iterations = []

    #Initialization step
    mu_k,sigma_k_sq,pi_k= inititialize_params(num_clusters)

    for iteration in range(num_iterations):
      lambda_ik,intermediate_term = expectation(mu_k, sigma_k_sq, pi_k, X)          #expectation_step
      pi_k,mu_k,sigma_k_sq = maximization(lambda_ik,X)                              #maximization_step
      mu_k, sigma_k_sq, pi_k = check_nans(mu_k, sigma_k_sq, pi_k)                   #exclusion of nan values         
      log_likelihood = get_log_likelihood(lambda_ik,intermediate_term)
      log_likelihood_iterations.append(log_likelihood)

    log_likelihood_iterations = np.array(log_likelihood_iterations).reshape(1,-1)
    mu_k = mu_k.reshape(1,-1)
    sigma_k_sq = sigma_k_sq.reshape(1,-1)

    hundred_iter_loglikelihood.append(log_likelihood_iterations)
    hundred_iter_mu_k.append(mu_k)
    hundred_iter_sigma_k_sq.append(sigma_k_sq)

  Log_Likelihood=np.mean(np.squeeze(np.array(hundred_iter_loglikelihood)),axis=0)
  Avg_mu_k=np.mean(np.squeeze(np.array(hundred_iter_mu_k)),axis=0)
  Avg_sigma_k=np.mean(np.squeeze(np.array(hundred_iter_sigma_k_sq)),axis=0)
  return Log_Likelihood, Avg_mu_k, Avg_sigma_k


def plot_loglikelihood(log_likelihood_iterations):
  plt.figure(figsize=(12,6))
  plt.plot(range(1, len(log_likelihood_iterations)+1), log_likelihood_iterations, marker='o')
  plt.title('Log-Likelihood vs Number of Iterations Plot for GMM')
  plt.xlabel('Number of iterations')
  plt.ylabel('Log-Likelihood')
  plt.grid()
  plt.show()


loglikelihood, mu_k, sigma_k_sq = EM_Algorithm(num_iterations, np_df)
plot_loglikelihood(loglikelihood)

print("mu-value (i.e. mean) of 4 Gaussian mixtures: ",mu_k)
print("sigma-sq-value (i.e. Variance) of 4 Gaussian mixtures: ",sigma_k_sq)