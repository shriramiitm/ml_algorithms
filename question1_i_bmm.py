# -*- coding: utf-8 -*-
"""Question1_i_BMM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Qvp3DaNJWec37hJH2uoR3dlN9-IX23-k
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.ticker as ticker
# import seaborn as sns
from numpy import linalg as LA

df = pd.read_csv("A2Q1.csv",header=None)
np_df=df.to_numpy()

#DATASET EXPLORING

# print(np_df[0:2,:])
mean=np_df.mean(axis=0)
centered_np_df=np_df-mean
cov_mat=(1/400.0)*np.dot(centered_np_df.T,centered_np_df)
eval, evec = LA.eig(cov_mat)
print("Eigen Values : \n",eval,"\n")
num_ones_prob=np_df.sum(axis=1)/df.shape[1]
print("Unique probability of number of ones in a data pt: \n",np.unique(num_ones_prob,return_counts=True)[0]) #This proves that there are only 15 unique p values implies that the points come from a bernoulli mixture
plt.figure(1)
plt_1=plt.figure(figsize=(12,6))
plt.plot(range(1,len(np.unique(num_ones_prob))+1),np.unique(num_ones_prob),marker="o")
plt.grid()
plt.ylabel("p-value")
plt.xlabel("number of unique p-values")
plt.title("Variation of p (i.e. bernoulli probability) in our dataset")
plt.show()

plt.figure(2)
plt_1=plt.figure(figsize=(12,6))
plt.plot(np.unique(num_ones_prob,return_counts=True)[0],np.unique(num_ones_prob,return_counts=True)[1],marker="o")
plt.grid()
plt.ylabel("number of datapoints")
plt.xlabel("p-value")
# plt.grid()
plt.title("Number of datapoints vs p-value")
plt.show()

##BERNOULLI MIXTURE MODEL

num_clusters = 4
num_iterations = 5

# Initializing params for EM algorithm
def inititialize_params(num_clusters):
  p_k = np.random.uniform(0.001, 1, num_clusters).reshape(1, -1)
  # p_k = np.around(p_k, 2)

  pi_k = np.random.dirichlet(np.ones(num_clusters)*1000, size=1)
  return p_k, pi_k


def update_lambda_ik(p_k_term,one_minus_p_k_term,pi_k):
  numerator= pi_k * p_k_term * one_minus_p_k_term
  denominator=np.sum(numerator, axis=1).reshape(-1, 1)
  lambda_ik_updated = numerator/denominator
  return lambda_ik_updated, numerator


# Expectation step
def expectation(p_k, pi_k, X):
  p_k_term = p_k ** X
  one_minus_p_k_term = (1 - p_k) ** (1 - X) 
  lambda_ik, intermediate_term =update_lambda_ik(p_k_term,one_minus_p_k_term,pi_k)
  return lambda_ik, intermediate_term


#function for updating pi_k used by maximization step
def update_pi_k(number_of_datapoints,lambda_ik):
  pi_k_updated = np.sum(lambda_ik, axis=0).reshape(1,-1)/ number_of_datapoints
  return pi_k_updated


#function for updating p_k used by maximization step
def update_p_k(lambda_ik, X):
  p_k_updated= (np.dot(X.T,lambda_ik)/np.sum(lambda_ik, axis=0).reshape(1,-1)).reshape(1,-1)
  return p_k_updated


# Maximization step: Updating parameters using lambdas calculated in Expectation step
def maximization(lambda_ik, X):
  num_datapts = X.shape[0]
  pi_k = update_pi_k(num_datapts,lambda_ik)
  p_k= update_p_k(lambda_ik, X)
  return pi_k,p_k


def get_log_likelihood(lambda_ik,intermediate_term):
  log_intermediate_term = np.log(np.sum(intermediate_term, axis=1))
  lambda_k=np.sum(lambda_ik,axis=1)
  log_likelihood = np.sum(lambda_k * (log_intermediate_term - np.log(lambda_k)))
  return log_likelihood


def EM_Algorithm(num_iterations,X,num_rand_initialisation=100):
  X = X.reshape(-1, 1)
  hundred_iter_loglikelihood = []
  hundred_iter_p_k = []
  
  for i in range(num_rand_initialisation):
    log_likelihood_iterations = []

    #Initialization step
    p_k,pi_k= inititialize_params(num_clusters)

    for iteration in range(num_iterations):
      lambda_ik,intermediate_term = expectation(p_k, pi_k, X)           #expectation_step
      pi_k,p_k = maximization(lambda_ik,X)                              #maximization_step
      log_likelihood = get_log_likelihood(lambda_ik,intermediate_term)
      log_likelihood_iterations.append(log_likelihood)

    log_likelihood_iterations = np.array(log_likelihood_iterations).reshape(1,-1)
    p_k = p_k.reshape(1,-1)

    hundred_iter_loglikelihood.append(log_likelihood_iterations)
    hundred_iter_p_k.append(p_k)

  Log_Likelihood=np.mean(np.squeeze(np.array(hundred_iter_loglikelihood)),axis=0)
  Avg_p_k=np.mean(np.squeeze(np.array(hundred_iter_p_k)),axis=0)

  return Log_Likelihood, Avg_p_k


def plot_loglikelihood(log_likelihood_iterations):
  plt.figure(figsize=(12,6))
  plt.plot(range(1, len(log_likelihood_iterations)+1), log_likelihood_iterations,marker='o')
  plt.title('Log-Likelihood vs Number of Iterations Plot for BNN')
  plt.xlabel('Number of iterations')
  plt.ylabel('Log-Likelihood')
  plt.grid()
  plt.show()


loglikelihood, p_k = EM_Algorithm(num_iterations, np_df)
plot_loglikelihood(loglikelihood)
# plt.grid()
print("p-value of 4 Bernoulli mixtures: ",p_k)