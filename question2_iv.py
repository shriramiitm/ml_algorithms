# -*- coding: utf-8 -*-
"""Question2_iv.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sLoMZbaesbKM5KpVTRC5qSSYSLz1XSBJ
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.metrics import r2_score

step_size=0.01
num_iterations=5000
num_folds=5

train_data = pd.read_csv("A2Q2Data_train.csv",header=None)
train_data_np=train_data.to_numpy()

test_data = pd.read_csv("A2Q2Data_test.csv",header=None)
test_data_np=test_data.to_numpy()

X_test=test_data_np[:,:100]
y_test=test_data_np[:,100]

X_train=train_data_np[:,:100]
y_train=train_data_np[:,100]

X=X_train.T
XXt=np.dot(X,X.T)
w_ML=np.dot(np.linalg.inv(XXt), np.dot(X,y_train))

def ridge_regression(train_X, train_y, lambda_) :

  num_datapts = train_X.shape[0]
  num_features = train_X.shape[1]
  # train_y=train_y.reshape(len(train_y),1)
  train_XtX = np.matmul(train_X.T, train_X)
  w_t = np.zeros((num_features))
  for i in np.arange(num_iterations):

    grad_f = ( 2*(np.matmul(train_XtX, w_t) - np.matmul(train_X.T, train_y)) + 2*lambda_*w_t  ) / num_datapts  
    w_t_plus_one = w_t - step_size * grad_f
    w_t = w_t_plus_one

  return w_t

def k_fold_cv(dataset, lambda_):
  
  dataset_copy = dataset.copy()
  np.random.shuffle(dataset_copy)
  folds = np.split(dataset_copy, num_folds, axis = 0)

  avg_err_folds = []
  for i in range(num_folds):
    
    testset = folds[i].copy()

    initialized = False
    for j in range(num_folds):
      if j == i:
        continue
      
      if initialized == False:
        trainset = folds[j].copy()
        initialized = True
      else:
        trainset = np.concatenate((trainset, folds[j]), axis=0)
    
    train_X = trainset[:, :-1]
    train_y = trainset[:, -1]
    model = ridge_regression(train_X, train_y, lambda_)

    test_X = testset[:, :-1]
    test_y = testset[:, -1]

    err = np.matmul(test_X, model) - test_y
    avg_err_folds.append(np.dot(err,err))

  return np.average(avg_err_folds)

#perfrom K fold cross validation
err_lambdas_after_cv = []
lambdas = np.linspace(0.01, 3, 40)

for lambda_ in lambdas:
  print('Lambda Value = ' + str(lambda_))
  error = k_fold_cv(train_data_np, lambda_)
  err_lambdas_after_cv.append(error)

#Get best lambda
lambda_index = np.argmin(err_lambdas_after_cv)
min_err_lambda = lambdas[lambda_index]

print("Minimum error lambda is = ", min_err_lambda)

#Get w_r for best lambda
w_r = ridge_regression(X_train, y_train, min_err_lambda)

#Calculate error for test data using W_ml & W_ridge
w_ML_err_vector = np.matmul(X_test, w_ML) - y_test
w_ridge_err_vector = np.matmul(X_test, w_r) - y_test

w_ML_err = np.dot(w_ML_err_vector.T, w_ML_err_vector)
w_ridge_err = np.dot(w_ridge_err_vector.T, w_ridge_err_vector)

print('Maximum Likelihood i.e. w_ML: \n \t r2 score= %.4f,\n \t test error= %.4f'%(r2_score(y_train, np.matmul(X_train, w_ML)), w_ML_err))
print('Ridge Regression i.e. w_ridge: \n \t r2 score= %.4f, \n \t test error= %.4f'%(r2_score(y_train, np.matmul(X_train, w_r)), w_ridge_err))

plt.figure(figsize=(8, 5))
plt.plot(lambdas, err_lambdas_after_cv, 'o-')
plt.title('Plot of Validation Error vs Lambda')
plt.xlabel('Lambda')
plt.ylabel('Validation Error')
plt.grid()
plt.show()

w_ML_err_vector_train = np.matmul(X_train, w_ML) - y_train
w_ridge_err_vector_train = np.matmul(X_train, w_r) - y_train

w_ML_err_train = np.dot(w_ML_err_vector_train, w_ML_err_vector_train)
w_ridge_err_train = np.dot(w_ridge_err_vector_train, w_ridge_err_vector_train)

print('w_ML train error: %.3f, \n w_r train error: %.3f'%(w_ML_err_train, w_ridge_err_train))