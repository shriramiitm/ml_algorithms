# -*- coding: utf-8 -*-
"""Question2_iii_SGD.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EFvYTUraDfExgHts1TI7fHkae5whslA_
"""

import numpy as np
import matplotlib.pyplot as plt
import math
import random
import pandas as pd


train_data = pd.read_csv("A2Q2Data_train.csv",header=None)
train_data_np=train_data.to_numpy()

test_data = pd.read_csv("A2Q2Data_test.csv",header=None)
test_data_np=test_data.to_numpy()

X_test=test_data_np[:,:100]
y_test=test_data_np[:,100].reshape(len(test_data_np),1)

X_train=train_data_np[:,:100]
y_train=train_data_np[:,100].reshape(len(train_data_np),1)

X=X_train.T
XXt=np.dot(X,X.T)
w_ML=np.dot(np.linalg.inv(XXt), np.dot(X,y_train))

#STOCHASTIC GRADIENT DESCENT

batch_size=100
Error_iter=[]
step_size=0.01
num_iterations=5000
num_datapts=len(y_train)
w_t=np.zeros_like(w_ML)

for iteration in range(num_iterations):

  random_index = np.random.randint(0, high=num_datapts, size=batch_size, dtype=int)
  Xb = X[:,random_index]
  yb = y_train[random_index,:]

  Xb=np.array(Xb)
  yb=np.array(yb)

  err=np.sqrt(np.dot((w_t-w_ML).T,(w_t-w_ML)))
  Error_iter.append(err)

  XbXbt=np.dot(Xb,Xb.T)
  grad_f= 2*(np.dot(XbXbt,w_t) - np.dot(Xb,yb))/batch_size

  w_t_plus_1 = w_t - step_size * grad_f
  w_t=w_t_plus_1

plt.figure(figsize=(12, 6))
plt.title(' Error ($|| w_t - w_{ML} ||_2$) Vs Number of iterations using SGD'  )
plt.plot(range(1,len(Error_iter)+1),np.squeeze(np.array(Error_iter)))
plt.xlabel('Number of Iterations')
plt.ylabel('Error = $|| w_t - w_{ML} ||_2$ ')
plt.grid()
plt.show()